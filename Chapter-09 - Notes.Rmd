---
title: "Chapter 9: ARIMA Models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fpp3)
```


__Basic Concept__: ARIMA models aim to describe the autocorrelations in the data


## 9.1 Stationarity and Differencing

__Stationarity__: a time series is considered stationary when its statistical
properties do not depend on the time at which it is observed. So, any timeseries
with seasonality or trend is not stationary. White noise is stationary.

__Differencing__: computing the differences between consecutive observations. Helps
stabilize the mean of the series. An ACF plot is useful for identifying a non-stationary
time series, but ndiffs() and nsdiffs() (for seasonal differencing) works too.

__Second Order Differencing__: if a series is not stationary after 1 difference,
it may need a second difference.

__Seasonal Differencing__: difference between an obsermation and the previous 
observation of the same season. Also termed "lag _m_ differences" where m is the 
seasonal period.

__Example__
```{r, fig.height=9, fig.width=8}
PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6) %>%
  transmute(
    `Sales ($million)` = Cost,
    `Log sales` = log(Cost),
    `Annual change in log sales` = difference(log(Cost), 12),
    `Doubly differenced log sales` =
                     difference(difference(log(Cost), 12), 1)
  ) %>%
  pivot_longer(-Month, names_to="Type", values_to="Sales") %>%
  mutate(
    Type = factor(Type, levels = c(
      "Sales ($million)",
      "Log sales",
      "Annual change in log sales",
      "Doubly differenced log sales"))
  ) %>%
  ggplot(aes(x = Month, y = Sales)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y") +
  labs(title = "Corticosteroid drug sales", y = NULL)
```

  1. Selecting which differences to apply is subjective
  2. If both seasonal and non-seasonal differencing is suggested, if the seasonal
  strength is high, perform seasonal first.
  3. Applying more differences than needed will introduce false dynamics. Perform
  as few as necessary.

### Unit Root Tests

The objective way to determine if differencing is required. They can be performed
with `unitroot_kpss` or `unitroot_ndiffs` or `unitroot_nsdiffs` in the `features()` 
function. To perform all of them use `features(Your_Data_Here, feature_set(pkgs = "feasts"))`

***


## 9.3 Autoregressive Models

Forecasting the variable of interest using a linear combination of past values 
of the variable.

__autoregression__: a regression of the variable against itself


AR models are restricted to stationary data

***


## 9.4 Moving Average Models

Instead of using previous values of the variable in a regression, a MA model
uses past forecast errors in a regression-like model.

Each observation of the data can be thought of as the weighted moving average of
the past few forecast errors.

Not to be confused with moving average smoothing from previous chapters. MA models
forecast future values while MA smoothing is for estimating trends of past values.

***


## 9.5 Non-Seasonal ARIMA Models

ARIMA combines autoregression, differencing, and moving average models. It stands 
for AutoRegressive Integrated Moving Average. This combination gives the ability to 
model the lagged values and lagged errors as well as make the series stationary.

ARIMA(p,d,q)

  * p: order of the autoregression
  * d: degree of differencing
  * q: order of moving average
  
`ARIMA()` from the `fable` package can select these parameters for you.

__Example__
```{r}
global_economy %>%
  filter(Code == "EGY") %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```
```{r}
fit <- global_economy %>%
  filter(Code == "EGY") %>%
  model(ARIMA(Exports))
report(fit)

```

```{r}
fit %>% forecast(h=10) %>%
  autoplot(global_economy) +
  labs(y = "% of GDP", title = "Egyptian Exports")
```

Above we see an ARIMA model of (2,0,1) captured the cyclic pattern over decades.


### ACF and PACF plots

In order to manually determine p,d,q starting points, employ ACF plots (for q) and 
PACF plots (for p), and then determining based on significant lags (above the blue line).

__ACF__ measures the relationship between $y_t$ and $y_{t-k}$ (lagged values)
__PACF__ measures the relationship between $y_t$ and $y_{t-k}$ after remove the 
effects of the lags in between. 

__Example__

```{r}
global_economy %>%
  filter(Code == "EGY") %>%
  gg_tsdisplay(Exports, plot_type = 'partial')
```

If you're going by significant lags, a (4,0,9) model may look like the 
recommendation here (last significant lag @ 4 w/ PACF and 9 with ACF), but keep 
this in mind:

ARIMA(p,d,0) may be warranted if:

  * ACF is exponentially decaying or sinusoidal
  * significant spike at lag _p_ in the PACF, but none beyond

ARIMA(0,d,q) may be warranted if:

  * PACF is exponentially decaying or sinusoidal
  * significant spike at lag _q_ in the ACF, but none beyond

Above we see a decaying sinusoidal in the ACF and the PACF shows the last
sig spike at 4. Starting with ARIMA(4,0,0) would be suggested. To select your
own ARIMA values for `ARIMA` to search within use:

`ARIMA(y ~ pdq(p=1:4, d=1, q=0:2))`

The (4,0,0) model does only slightly worse than the Auto ARIMA function  
(with an AICc value of 294.70 compared to 294.29).

***


## 9.6 Estimation and Order Selection

__MLE__: Maximum Likelihood Estimation finds the values of the parameters with 
maximize the probability of obtaining the original series.

ARIMA models are much more complicated to estimate than regression models, and
estimation can vary based on the algorithm used.

`fable` will maximize the log likelihood of the data; the log of the probability of the 
observed data coming from the model.


__Information Criteria__
AIC, AICc, and BIC are also used (AICc preferred by the textbook) to determine
best orders of p and q, but due to differencing changing the data on which log
likelihood is computed, Information Criteria cannot be compared between models
with different d values.


***


## 9.7 ARIMA Modeling in `fable`

#### How Does `ARIMA()` Work?



***

## 8.7 Forecasting with ETS Models



***


