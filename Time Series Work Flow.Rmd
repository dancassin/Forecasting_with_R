---
title: "Time Series Workflow"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(feasts)
library(grid)
library(gridExtra)
library(fpp3)
```

## Load Data Set

A univariate time series is required for this notebook, and should be saved to "data".
Time series should be cleaned and ready to analyze and forecast. The name of the 
column you are interested in should be formatted as a string and pointing to "column".


```{r}
# 1. Upload a time series 
# 2. Specify the column you would like to work with

# Examples provided



data <- us_employment %>%
  filter(Title == "Leisure and Hospitality",
         year(Month) > 2000) %>%
  mutate(Employed = Employed/1000) %>%
  select(Month, Employed)

column <- 'Employed'

# data <- PBS %>%
#   filter(ATC2 == "A10") %>%
#   select(Month, Scripts) %>%
#   summarize(Total_Scripts = sum(Scripts))
# 
# column <- 'Total_Scripts'


data
```



## Split Data into Train Test

In order to statistically test the accuracy of different models, an 80/20 train/test
split is employed. It is required to know the frequency of the observations. Daily and 
weekly time series not included due to seasonal frequencies being 365 and 52, respectively.
These periods are usually too large for the models here, and are better suited
for transformation to the periods below or Fourier transforms (which are not 
covered in this notebook). 

```{r}
train_test_split <- function(dataframe, timeperiod, percentTrain = 0.8) {
  # timeperiod = ['yearly', 'quarterly', 'monthly', 'business_daily']
  # assumed 80/20 split
  
  df_length = dim(dataframe)[1]
  index = round(df_length * percentTrain)
  
  train = dataframe %>%
    slice(1:index)
  
  test = dataframe %>%
    slice(index+1:df_length)

  return(list(train, test))
}

train_test <- train_test_split(data, monthly, 0.8)

train <- train_test[[1]]
test <- train_test[[2]]
```



## Visualize Data

One of the first steps of any time series project is to visualize your data so
that you may plan your course of action.

```{r}
visualize_timeseries <- function(data, column){
  col_name <- as.name(column)
  
  data %>%
    autoplot(!!col_name)

} 

visualize_timeseries(train, column)
```

## Time Series Stats

The stats returned will guide in understanding the time series and in
determining future processes.

```{r}
ts_stats <- function(single_timeseries, column) {
  
  col_name <- as.name(column)

  ts_features <- single_timeseries %>%
    features(!!col_name, feature_set(pkgs = "feasts")) 
  
  ts_features <- ts_features %>%
    select(trend_strength,
           lambda_guerrero,
           ndiffs,
           nsdiffs,
           lb_stat,
           lb_pvalue,
           coef_hurst,
           spectral_entropy
           )
  
  if(ts_features$ndiffs > 0) {
    print('Data is not stationary')
  }
  else{
    print('If no seasonal differencing suggested, data is stationary.')
  }
  
  if(ts_features$nsdiffs > 0){
    print('Data is Seasonal and may require seasonal differencing')
  }
  else{
     print('No seasonality detected')
  }

  if(ts_features$lb_pvalue < 0.05) {
    print('Data is differentiated from white noise')
  }
  else
    print('Data is not differentiated from white noise')
  
  if(ts_features$coef_hurst > .75) {
    print('Hurst coefficient states: Time series has long memory; thus significant autocorrelations for many lags')
  }
  else
    print('Hurst coefficient states: may have a few significant autocorrelations, but not for many lags')
  
  if(ts_features$spectral_entropy > .5) {
    print('Spectral entropy approaching 1: states time series will be difficult to forecast')
  }
  else
    print('Spectral entropy approaching 0: states time series will be easier to forecast')

  return(ts_features)
}

train_stats <- ts_stats(train, column)

train_stats
```


## Consider Transformations
  
  * Calendar Adjustments 
      * Trends or seasonality may not be immediately noticeable in the initial
      time period. 
      * Holidays and weekends impact stock market data.
  * Population Adjustments
      * Geographic density changes over time. Per capita adjustments are typically 
      more truthful.
  * Inflation Adjustments
      * The value/buying power of currency changes over time
  * Mathematical Transformations:
      * Box Cox transform uses the optimal lambda via the Guerrero method from the
      stats tibble above. Method is helpful when the variance of the time series is 
      consistently increasing over time.

#### Comparison of Original to Transform Using Lambda
```{r}
transform_comparison <- function(dataframe, column, stats_df, append = FALSE, lambda = NULL){
  # Function derives the Box Cox transform and adds it to the original data frame
  # if append = True. This retains the tsibble format for the transform.
  # Function plots both original and transform for comparison.

  col_name <- as.name(column)
  
  if (is.null(lambda)) {
    dataframe['transform'] <- BoxCox(dataframe[, {{ column  }}], lambda = stats_df$lambda_guerrero)
  }
  else{
    dataframe['transform'] <- BoxCox(dataframe[, {{ column  }}], lambda = lambda)
  }
  

  p1 <- dataframe %>%
    autoplot(!!col_name) +
    labs(y = 'Original')
  
  p2 <- dataframe %>%
    autoplot(transform) +
    labs(y = 'Box Cox Transform')
  
  grid.arrange(p1, p2, nrow = 2)
  
  if(append == TRUE) {
    return(dataframe)
  }
}

transform_comparison(train, column, train_stats, append = TRUE)

```


#### If Box Cox Transform Acceptable...

Un-comment below code to transform data and henceforth point the transformed 
data to the "column" variable.

```{r}

# data <- transform_comparison(data, column, train_stats, append = TRUE)
# test <- transform_comparison(test, column, train_stats, append = TRUE)
# train <- transform_comparison(train, column, train_stats, append = TRUE)
# column <- 'transform'
```




## Discovering Seasonality

For data with seasonality, the below produces a plot to help uncover seasonal changes    
```{r fig.align="center", echo = FALSE,fig.width = 14}
seasonal_plots <- function(dataframe, column, time_period, line_labels = NULL) {
  # time_period for gg_season takes 'day', 'week', 'month','year', and NULL if
  # you prefer the function to choose for you. This time period is the comparitive
  # scope, so it is one metric bigger than an observation. Example: if the data is
  # hourly, choose day; if monthly choose year.Quarter is not an option.
  
  col_name <- as.name(column)

  p1 <- dataframe %>%
    gg_season(!!col_name, period = time_period, labels = line_labels) +
    labs(title = 'Seasonal Plot')
  
  p2 <- dataframe %>%
    gg_subseries(!!col_name) +
    labs(title = 'Seasonal Subseries')
  
  p3 <- dataframe %>%
    gg_lag(!!col_name, geom = 'point', lags = 1:12, ) +
    #facet_grid(rows=4, cols=3) +
    facet_wrap(~ .lag, ncol=3) +
    theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
  
  grid.arrange(p1, p2, p3, layout_matrix = rbind(c(1,3), c(2,3)))
}

seasonal_plots(train, column, time_period = 'year', line_labels = 'both')
```

## Decomposition

Splitting a time series into trend, seasonality, and remainder for further inspection.
```{r}
# RJ Note: I would suggest making window, period, degree, and jump parameters 
# variable, but not sure if you left them out for some other purpose.

decomp <- function(data, column){
  col_name <- as.name(column)
  
  data %>%
    model(STL((!!col_name) ~ trend(window = 7) + season(window = "periodic"), robust = TRUE)) %>%
    components() %>%
    autoplot()
}


decomp(train, column)
```



## Establish Benchmark Models

This section determines the models to use based on the `train_stats` dataframe and 
fits benchmark models to test the more complicated models against.

```{r}

benchmark_models <- function(dataframe, column, df_stats) {

  col_name <- as.name(column)
  
  # Seasonal with Drift
  if (df_stats$trend_strength > .70 & df_stats$nsdiffs > 0) {
    df_fit <- dataframe %>%
      model(
        Mean = MEAN(!!col_name),
        `Naïve` = NAIVE(!!col_name),
        Drift = NAIVE((!!col_name) ~ drift()),
        `Seasonal Naïve` = SNAIVE(!!col_name),
      )
  }
  # Seasonal Only
  else if (df_stats$trend_strength < .70 & df_stats$nsdiffs > 0){
    df_fit <- dataframe %>%
      model(
        Mean = MEAN(!!col_name),
        `Naïve` = NAIVE(!!col_name),
        `Seasonal Naïve` = SNAIVE(!!col_name)
        )
  }
  # Drift Only
  else if (df_stats$trend_strength > .70 & df_stats$nsdiffs < 1) {
    df_fit <- dataframe %>%
      model(
        Mean = MEAN(!!col_name),
        `Naïve` = NAIVE(!!col_name),
        Drift = NAIVE((!!col_name) ~ drift())
        )
  }
  # No Seasonality or Trend
  else
    df_fit <- dataframe %>%
      model(
        Mean = MEAN(!!col_name),
        `Naïve` = NAIVE(!!col_name),
    )

  
  return(df_fit)
}

benchmark_fit <- benchmark_models(train, column, train_stats)

benchmark_fit
```


#### Plot Benchmark Models and Get Accuracy Scores

```{r}
plot_benchmark_models <- function(dataframe, column, test_set, df_fit, zoom_pct = 0.0){
  # zoom pct takes range from 0.0 - 0.8
  col_name <- as.name(column)
  
  test_set_length <- dim(test_set)[1]
  
  data_length <- dim(dataframe)[1]
  zoom_point <- round(data_length * zoom_pct)
  truncated_range <- dataframe[zoom_point:data_length,]
  
  
  fc <- df_fit %>% forecast(h=test_set_length)
  
  benchmark_accuracy <- accuracy(fc, test_set)
  
  benchmark_plot <- fc %>%
    autoplot(truncated_range, level = NULL) +
    labs(title = "Benchmark Models") +
    autolayer(test_set, .vars=!!col_name)

  return(list(benchmark_accuracy, benchmark_plot))
}

benchmark_results <- plot_benchmark_models(data, column, test, benchmark_fit, zoom_pct = .5)

benchmark_accuracy <- benchmark_results[[1]]

benchmark_results
```


#### Plot Best Benchmark Model's Residuals

Function uses RMSE to sort the dataframe and return best performing benchmark.
```{r}
plot_best_benchmark_residuals <- function(model_accuracy, dataframe, column) {
  col_name <- as.name(column)
  
  benchmark_data <- model_accuracy %>% arrange(RMSE)
  
  print(benchmark_data[1,])
  
  best_benchmark <- benchmark_data[1,1]
  
  if (best_benchmark == 'Seasonal Naïve') {
    data %>%
      model(`Seasonal Naïve` = SNAIVE(!!col_name)) %>%
      gg_tsresiduals()
  } else if (best_benchmark == 'Drift') {
    data %>%
      model(Drift = NAIVE((!!col_name) ~ drift())) %>%
      gg_tsresiduals()
  } else if (best_benchmark == 'Naïve') {
    data %>%
      model(`Naïve` = NAIVE(!!col_name)) %>%
      gg_tsresiduals()
  } else if (best_benchmark == 'Mean') {
    data %>%
      model(Mean = MEAN(!!col_name)) %>%
      gg_tsresiduals()
  }
  
  
}

plot_best_benchmark_residuals(benchmark_accuracy, data, column)
```

## Testing More Advanced Models

#### Linear Model Based on Trend and/or Seasonality

Fitting a Time Series Linear Model using trend and seasonality as predictors.

```{r}
get_residuals <- function(model_fit){
  lb_result <- augment(model_fit) %>%
    features(.innov, ljung_box, lag = 10)
  
  print(lb_result)
  
  if (lb_result$lb_pvalue < 0.05) {
    print('Series is differentiated from white noise.')
  } else {
    print('Series is not differentiated from white noise.')
  }
  gg_tsresiduals(model_fit)
  
  # return(lb_result)
}
```


```{r}
fit_trend_seasonal_lm <- function(train_set, column, train_stats, test_set) {
  col_name <- as.name(column)
  
  test_set_length <- dim(test_set)[1]
  
  if (train_stats$trend_strength > .75 & train_stats$nsdiffs > 0) {
    fit_data <- train_set %>%
      model(TSLM((!!col_name) ~ trend() + season()))
  } else if (train_stats$trend_strength < .75 & train_stats$nsdiffs > 0) {
    fit_data <- train_set %>%
      model(TSLM((!!col_name) ~ season()))
  } else if (train_stats$trend_strength > .75 & train_stats$nsdiffs < 1) {
    fit_data <- train_set %>%
      model(TSLM((!!col_name) ~ trend()))
  }else{print("No trend or seasonality to predict")}
  
  report(fit_data)
  
  fc <- fit_data %>% forecast(h=test_set_length)
  
  lm_accuracy <- (accuracy(fc, test_set))
  
  fc_chart <- fc %>%
    autoplot(train_set, level = NULL) +
    labs(title = "Linear Model Forecast on Test Set") +
    autolayer(test_set, .vars=!!col_name)
  
  #residual_chart <- fit_data %>% gg_tsresiduals()
  
  return(list(fit_data, lm_accuracy, fc_chart))
} 


lm_output <- fit_trend_seasonal_lm(train, column, train_stats, test)

lm_output[[3]]

lm_accuracy <- lm_output[[2]]
lm_accuracy

lm_fit <- lm_output[[1]]
```


```{r}
get_residuals(lm_fit)

```

#### ETS Model

Exponential Smoothing weights observations in an exponentially decreasing fashion
starting with the most recent observation. This gives the most recent observations
more weight in forecasting.
```{r}
ets_model <- function(train_set, column, test_set) {
  col_name <- as.name(column)
  
  ets_fit <- train_set %>%
    model(ETS(!!col_name))

  ets_report <- report(ets_fit)
  
  ets_fc <- ets_fit %>%
    forecast(h=dim(test_set)[1])
  
  fc_accuracy <- accuracy(ets_fc, test_set)
  
  ets_plot <- ets_fc %>%
    autoplot(train_set, level = NULL) +
    labs(title = "ETS Model") +
    autolayer(test, .vars=!!col_name)

  return(list(ets_fit, ets_report, fc_accuracy, ets_plot))  
}

ets_output <- ets_model(train, column, test)

ets_fit <- ets_output[[1]]
ets_report <- ets_output[[2]]
ets_accuracy <- ets_output[[3]]

ets_fit
ets_report
ets_accuracy
ets_output[[4]]
```



```{r}
get_residuals(ets_fit)
```


#### ARIMA Models

ARIMA models use weighted moving averages of past forecast errors, differencing 
for stationarity, and autoregression (a regression model using a linear combination 
of past values of the variable) to produce a forecast.

The option below uses R's auto ARIMA function with `stepwise` and `approx` args both
equaling `FALSE` to force it to evaluate all reasonable models. This takes longer 
but is more thorough.

```{r}

arima_model <- function(train_set, column, test_set) {
  col_name <- as.name(column)
  
  arima_fit <- train %>%
    model(`ARIMA` = ARIMA(!!col_name, stepwise=FALSE, approx=FALSE))

  arima_report <- report(arima_fit)
  
  arima_fc <- arima_fit %>%
    forecast(h=dim(test_set)[1])
  
  arima_accuracy <- accuracy(arima_fc, test_set)
  
  arima_plot <- arima_fc %>%
    autoplot(train_set, level = NULL) +
    labs(title = "ARIMA Model") +
    autolayer(test, .vars=!!col_name)

  return(list(arima_fit, arima_accuracy, arima_plot))  
}

arima_output <- arima_model(train, column, test)

arima_fit <- arima_output[[1]]

arima_accuracy <- arima_output[[2]]

arima_output
```


```{r}
get_residuals(arima_fit)
```

## Assess Model Forecast Performance

Below cell combines all accuracy tables and arranges the best model at the top 
by lowest RMSE.
```{r}
bind_rows(
  benchmark_accuracy, 
  lm_accuracy,
  ets_accuracy,
  arima_accuracy) %>%
  arrange(RMSE)
```
